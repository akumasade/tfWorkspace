{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow: Neural Networks\n",
    "*Rachel Buttry*\n",
    "\n",
    "*16 April 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that tensorflow is capable of performing the basic tasks that python can (with some longer syntax), but where it really shines is when doing deep learning. Tensorflow provides tools, such as tensoboard, that may not be particularly useful for doing sklearn models from scratch, but are very helpful when designing and using artificial neural networks (ANN).\n",
    "\n",
    "This notebook with give overviews of deep learning concepts, but I highly reccommend reading [this article](http://adventuresinmachinelearning.com/neural-networks-tutorial/) for a more in-depth introduction to the subject. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our data. For this example, we'll be using the [MNIST digits dataset](http://yann.lecun.com/exdb/mnist/). Code for this example was taken directly from: [Adventures in Machine Learning](http://adventuresinmachinelearning.com/python-tensorflow-tutorial/?sfw=pass)\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png' width='50%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load mnist digits\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an Aritifical Nerual Network?\n",
    "An articficial nerural network is a machine learning model with a structure that is based off of the brain. The \"neurons\" (a.k.a nodes) of a given layer are connected to the \"neurons\" of the next layer. The \"neurons\" take the input, weigh it, then use an activation function to determine if it should \"fire\" or just return zero. Artificial neural networks can be designed and trained for very specific problems.\n",
    "Image from [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network):\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/e/e4/Artificial_neural_network.svg' width='25%'>\n",
    "\n",
    "We're going to make simple neural net with an input layer, one hidden layer, and an output layer to apply to our dataset. It will have the shape (784-300-10) where there are 784 nodes in the first (input) layer, 300 in the second (hidden), and 10 in the third (output) layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "The MNIST images are 28x28 pixel each where each pixel has a grayscale value from 0 to 1. \n",
    "\n",
    "28 x 28 = 784\n",
    "\n",
    "Thus our input layer will have 784 input nodes.\n",
    "(Note we're also creating a placeholder for the correct labels of the data that isn't part of the input layer. This is so we can compare the neural net output to the correct label.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the training data placeholders\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# now declare the output data placeholder - 10 digits\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer\n",
    "There are 300 nodes in our hidden layer. It's a somewhat aribitrary number, but for this example, why not?\n",
    "\n",
    "In reality, there are a bunch of conventions and \"rules\" for determining the ideal number of hidden layers you need as well as how many nodes in should be in each hidden layer. For instance, you generally want the number of nodes in the hidden layer to be between the number of input nodes and the number of output nodes.  We won't worry too much about all the conventions for now and just say that since 784 > 300 > 10, we should be good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and Bias\n",
    "For each layer, there is a set of weights $W$ multiplied by the input values and every input value (once multiplied by the appropriate weight) has a bias value $b$ added to it. This new  set of values will become the input values for our hidden layer activation function.\n",
    "\n",
    "<center>\n",
    "    $input \\cdot W   + b = weighted \\space input$\n",
    "</center>\n",
    "\n",
    "\n",
    "* $W$ is a matrix of shape $inputsize \\times \\# nodes$\n",
    "* $b$ is a vector of length $\\# nodes$.\n",
    "\n",
    "So, our $W$ for this layer is  of shape $784 \\times 300$ and $b$ is a vector of length $300$. Notice that we're multiplying the input data on the right so that our $1 \\times 784$ vector ultimately becomes a $1 \\times 300$ vector. This exact order of multiplication and shapes of the weights/biases aren't really a convention, the goal is to match the data inputed with the respective weights/biases and get back the shape we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now declare the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([300]), name='b1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions\n",
    "In a neuron, an activation function is a function to determine whether the neuron should fire (activate) or not. In a neural network, the activation function works in a similar way. However, rather than a binary fire vs. don't fire, the activation function will usually return a continuous value (usually between 0 and 1) that can be thought of as a partial firing.\n",
    "\n",
    "The input for this function is the weighted/biased data from the input layer:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    out = f(W \\cdot input + b)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Every node in the layer uses the same activation fucntion. We could hypothetically loop thru the nodes of a given layer and use different activation fucntions on each, but that is computationally costly and there isn't any clear benefit to doing so.\n",
    "\n",
    "In this layer we're using the [rectifier linear unit activation function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f(x) = max(0, x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The function returns the value of the input if it's positive, and zero otherwise.\n",
    "It is called with ```tf.nn.relu()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the output of the hidden layer\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "The output of our hidden layer becomes the input for our last layer.\n",
    "\n",
    "For this layer, we use the [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f(x_k) = \\frac{e^{x_k}}{\\sum_{i=1}^{n} e^{x_i}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "* $k$ is the number of the current output node\n",
    "* $n$ is the number of output nodes\n",
    "\n",
    "The function returns a value between 0 and 1 that is the probability of the node's outcome. It is called using ```tf.nn.softmax()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "# now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "# output layer\n",
    "y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Fucntion\n",
    "We want to minimize the error between our predicted and correct values. To do this, we represent the error using a cost (a.k.a loss) function and then minimize it. We're using the [cross entropy cost function](https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "\n",
    "We have 10 output nodes so that each will (ideally) return either a 1 or a zero. So if the input image is a 6, then we'd expect the correct label to look like \\[0,0,0,0,0,0,1,0,0,0\\]. This means that we'd only want the output node corresponding to the number 6 to return a 1 and the rest to return 0. (This type of representation is called [one-hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f).)\n",
    "\n",
    "Each node should return a binary value (0 or 1), so we use the binary calculation for cross entropy. For each test prediction output node $i$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    L_i = -(y_{actual}\\space log(y_{pred}) + (1-y_{actual})log(1 - y_{pred})) \n",
    "\\end{aligned}\n",
    "$$\n",
    "* $y_{actual}$ is the actual label value for that node\n",
    "* $y_{pred}$ is the value predicted for the test data by the neural net\n",
    "\n",
    "We want to find the total loss for a given batch, then calculate the average loss across all batches:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    L_{avg} = \\frac{1}{m} \\sum_{j=1}^{m}\\sum_{i=1}^{n}L_{i,j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "* $L_i^m $ is the Loss for node $i$ in batch $j$\n",
    "* $m$ is the number of batches\n",
    "* $n$ is the number of output nodes\n",
    "\n",
    "\n",
    "This is the function we are going to minimize using gradient descent.\n",
    "\n",
    "**Note:** We're going to clip the y value as to avoid $NaN$s when taking $log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                         + (1 - y) * tf.log(1 - y_clipped), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Terminology\n",
    "* **forward pass** - computes values from inputs to output\n",
    "* **backward pass** - backpropagation which starts at the end and recursively applies the chain rule to compute the gradients\n",
    "* **pass** - one forward pass and one backward pass\n",
    "* **batch size** - number of examples in one forward/backward pass\n",
    "* **number of iterartions** - number of passes, each pass using *batch size* number of examples\n",
    "* **epoch** - one forward pass and backward pass of *all* training samples\n",
    "* **learning rate** - how much the coefficents can change with each update\n",
    "\n",
    "For instance, if you have 1000 training examples and a batch size of 500, then it will take 2 iterations to complete 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python optimisation variables\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "optimiser = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Backpropogation\n",
    "Ordinarily when applying gradient descent to a neural network, the derivatives are applied in the direction ouput to input. The chain rule is applied to the final output, thus partial derivatives of the gradient need to be calculated \"from end to start\".\n",
    "\n",
    "This is known as [backpropogation](https://brilliant.org/wiki/backpropagation/)--short for \"backward propagation of errors\". Tensorflow has made it so that we don't need to write our own backpropogation algorithm. The ```GradientDescentOptimizer()``` will minimize loss and do all the hard work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , cost = 0.603\n",
      "Epoch: 2 , cost = 0.224\n",
      "Epoch: 3 , cost = 0.164\n",
      "Epoch: 4 , cost = 0.126\n",
      "Epoch: 5 , cost = 0.108\n",
      "Epoch: 6 , cost = 0.088\n",
      "Epoch: 7 , cost = 0.073\n",
      "Epoch: 8 , cost = 0.061\n",
      "Epoch: 9 , cost = 0.051\n",
      "Epoch: 10 , cost = 0.041\n"
     ]
    }
   ],
   "source": [
    "# finally setup the initialisation operator\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# initialise the variables\n",
    "sess.run(init)\n",
    "total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "\n",
    "# run the training\n",
    "for epoch in range(epochs):\n",
    "    avg_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "        _, c = sess.run([optimiser, cross_entropy], \n",
    "                     feed_dict={x: batch_x, y: batch_y})\n",
    "        avg_cost += c / total_batch\n",
    "    print \"Epoch:\", (epoch + 1), \", cost =\", \"{:.3f}\".format(avg_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9772\n"
     ]
    }
   ],
   "source": [
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Notes\n",
    "There's definetly room for improvement, but 97% accuracy is a good enough for our little nerural net. \n",
    "\n",
    "It's important to be aware othat we won't be able to understand the meaning of what our model is doing. If we wanted to, we could extract the weights and biases, but those are just numbers and don't have any physical meaning. As neural networks become more and more complex, the individual weights/biases will be more difficult to interpret.\n",
    "\n",
    "So, if you're looking for a machine learning model that you will be able to easily understand where the result came from, neural networks are *not* for you.\n",
    "\n",
    "#### Additional Resources:\n",
    "* [Activation Functions Intro](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)\n",
    "* [Convolutional Neural Networks Intro](http://cs231n.github.io/optimization-2/)\n",
    "* [Loss Functions Cheatsheet](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
