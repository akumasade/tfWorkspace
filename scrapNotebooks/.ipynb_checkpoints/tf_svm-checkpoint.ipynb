{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow: Linear SVM (applied to credit screening data)\n",
    "*Rachel Buttry*\n",
    "\n",
    "*April 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suport Vector Machine (SVM)\n",
    "SVM is a method used specifically for binary classification. It calculates a \"boundary line\" (really a hyperplane) that ideally seperates the provided data into the two possible classes. Image from [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "<img src=\"./Svm_separating_hyperplanes_(SVG).svg\" width=\"50%\">\n",
    "\n",
    "Below is a slightly modified version of the code from [nfcclure's tensorflow cookbook](https://github.com/nfmcclure/tensorflow_cookbook/blob/master/04_Support_Vector_Machines/02_Working_with_Linear_SVMs/02_linear_svm.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was originally taken from [UCI Machine Learning Repo](https://archive.ics.uci.edu/ml/datasets/Credit+Approval).\n",
    "\n",
    "It was recoded by the script ```recode_crx.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "crx = pd.read_csv(\"./crx-recoded.csv\",delimiter=\",\")\n",
    "\n",
    "ytemp = crx.pop('A16')\n",
    "y_vals = (ytemp.as_matrix()).astype(float)# 0,1 for approved,rejected\n",
    "x_vals = (crx.as_matrix()).astype(float)#survey data (recoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]\n",
    "#iris = datasets.load_iris()\n",
    "#x_vals = np.array([[x[0], x[3]] for x in iris.data])\n",
    "#y_vals = np.array([1 if y == 0 else -1 for y in iris.target])\n",
    "\n",
    "#split into train/test\n",
    "np.random.seed(1234)\n",
    "train_indices = np.random.choice(len(x_vals),\n",
    "                                 int(round(len(x_vals)*0.8)),\n",
    "                                 replace=False)\n",
    "test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
    "x_vals_train = x_vals[train_indices]\n",
    "x_vals_test = x_vals[test_indices]\n",
    "y_vals_train = y_vals[train_indices]\n",
    "y_vals_test = y_vals[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also apply the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method) to create a more appropriate boundary. For this example though, we'll restrict ourselves to just use the linear kernal--meaning we're just going to try to seperate them  with a straight line/\"flat\" hyperplane. It has a loss function:\n",
    "<center>\n",
    "$Loss = \\frac{||A||^{2}}{2} + \\alpha \\cdot \\frac{1}{n} \\sum_{n=1}^{N}max(0, 1 -( A \\cdot x + b))$\n",
    "\n",
    "</center>\n",
    "\n",
    "Where \n",
    "* $A$ is the normal vector of the hyperplane\n",
    "* $b$ is the offset of the hyperplane\n",
    "* $\\alpha$ is the soft margin parameter (how much we allow for missclassification)(?)\n",
    "\n",
    "So $A$ and $b$ decribes the hyperplane that we're solving for. This loss fucntion is a sum of a quadratic loss function and [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create linear svm\n",
    "\n",
    "# Declare hyperplane dimensions\n",
    "numcols = np.shape(x_vals)[1]\n",
    "Ashape = [numcols, 1]\n",
    "bshape = [1, 1]\n",
    "\n",
    "# Declare batch size\n",
    "batch_size = len(x_vals_train)\n",
    "\n",
    "# Initialize placeholders\n",
    "#x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)# depends on shape of data\n",
    "#y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)# ||\n",
    "\n",
    "x_data = tf.placeholder(shape=[None, numcols], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Create variables for SVM\n",
    "#A = tf.Variable(tf.random_normal(shape=[2, 1]))# depends on shape of data\n",
    "#b = tf.Variable(tf.random_normal(shape=[1, 1]))# depends on shape of data\n",
    "\n",
    "A = tf.Variable(tf.random_normal(shape=Ashape))# depends on shape of data\n",
    "b = tf.Variable(tf.random_normal(shape=bshape))# depends on shape of data\n",
    "\n",
    "# Declare model operations\n",
    "model_output = tf.subtract(tf.matmul(x_data, A), b)\n",
    "\n",
    "# Declare vector L2 'norm' function squared\n",
    "l2_norm = tf.reduce_sum(tf.square(A))\n",
    "\n",
    "# Declare loss function\n",
    "# L2 regularization parameter, alpha\n",
    "alpha = tf.constant([0.01])\n",
    "# Margin term in loss\n",
    "classification_term = tf.reduce_mean(tf.maximum(0., tf.subtract(1., \\\n",
    "                            tf.multiply(model_output, y_target))))\n",
    "# Put terms together\n",
    "loss = tf.add(l2_norm/2, tf.multiply(alpha, classification_term))\n",
    "\n",
    "# Declare prediction function\n",
    "prediction = tf.sign(model_output)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target), tf.float32))\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = my_opt.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100\n",
      "Loss = [1.6352828]\n",
      "Step #200\n",
      "Loss = [0.19222452]\n",
      "Step #300\n",
      "Loss = [0.03237231]\n",
      "Step #400\n",
      "Loss = [0.01103274]\n",
      "Step #500\n",
      "Loss = [0.00874015]\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training loop\n",
    "loss_vec = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "for i in range(500):\n",
    "    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n",
    "    rand_x = x_vals_train[rand_index]\n",
    "    rand_y = np.transpose([y_vals_train[rand_index]])\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss) #for plotting\n",
    "\n",
    "    train_acc_temp = sess.run(accuracy, feed_dict={\n",
    "        x_data: x_vals_train,\n",
    "        y_target: np.transpose([y_vals_train])})\n",
    "    train_accuracy.append(train_acc_temp)#for plotting\n",
    "\n",
    "    test_acc_temp = sess.run(accuracy, feed_dict={\n",
    "        x_data: x_vals_test,\n",
    "        y_target: np.transpose([y_vals_test])})\n",
    "    test_accuracy.append(test_acc_temp)#for plotting\n",
    "    \n",
    "    # for printing\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print('Step #' + str(i+1))\n",
    "        print('Loss = ' + str(temp_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =  [[ 0.01046679  0.03551899  0.00340231  0.00846251  0.01328621 -0.00036537\n",
      "  -0.00031592 -0.00247209  0.02757664  0.01122418 -0.01167194 -0.00010599\n",
      "   0.00500354  0.00042625  0.00451804 -0.00352069  0.00781336 -0.00555783\n",
      "  -0.00267743  0.00717988 -0.00147304  0.00316507 -0.00456573  0.00692532\n",
      "  -0.00255281]]\n",
      "b =  [[2.8120515]]\n",
      "Accuracy: 0.35507247\n"
     ]
    }
   ],
   "source": [
    "Avalue = sess.run(A)\n",
    "bvalue = sess.run(b)\n",
    "print \"A = \", np.transpose(Avalue)\n",
    "print \"b = \", bvalue\n",
    "print \"Accuracy:\", test_accuracy[-1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract coefficients\n",
    "[[a1], [a2]] = sess.run(A)\n",
    "[[b]] = sess.run(b)\n",
    "slope = -a2/a1\n",
    "y_intercept = b/a1\n",
    "\n",
    "# Extract x1 and x2 vals\n",
    "x1_vals = [d[1] for d in x_vals]\n",
    "\n",
    "# Get best fit line\n",
    "best_fit = []\n",
    "for i in x1_vals:\n",
    "    best_fit.append(slope*i+y_intercept)\n",
    "\n",
    "# Separate I. setosa\n",
    "setosa_x = [d[1] for i, d in enumerate(x_vals) if y_vals[i] == 1]\n",
    "setosa_y = [d[0] for i, d in enumerate(x_vals) if y_vals[i] == 1]\n",
    "not_setosa_x = [d[1] for i, d in enumerate(x_vals) if y_vals[i] == -1]\n",
    "not_setosa_y = [d[0] for i, d in enumerate(x_vals) if y_vals[i] == -1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Plotting \n",
    "\n",
    "%matplotlib inline\n",
    "# Plot data and line\n",
    "plt.plot(setosa_x, setosa_y, 'o', label='I. setosa')\n",
    "plt.plot(not_setosa_x, not_setosa_y, 'x', label='Non-setosa')\n",
    "plt.plot(x1_vals, best_fit, 'r-', label='Linear Separator', linewidth=3)\n",
    "plt.ylim([0, 10])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Sepal Length vs Pedal Width')\n",
    "plt.xlabel('Pedal Width')\n",
    "plt.ylabel('Sepal Length')\n",
    "plt.show()\n",
    "\n",
    "# Plot train/test accuracies\n",
    "plt.plot(train_accuracy, 'k-', label='Training Accuracy')\n",
    "plt.plot(test_accuracy, 'r--', label='Test Accuracy')\n",
    "plt.title('Train and Test Set Accuracies')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over time\n",
    "plt.plot(loss_vec, 'k-')\n",
    "plt.title('Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scikit learn way\n",
    "Alright, we had some really bad accuracy for the tensorflow model, but is it an issue with the model or the dataset? Let's find out by comparing it to the scikit learn model!\n",
    "\n",
    "Code based off of [PHYS T480: Classification2 Notebook](https://github.com/gtrichards/PHYS_T480/blob/master/Classification2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(x_vals_train, y_vals_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[  0.    58.67   4.46   3.04   1.     1.     6.     0.    43.   560.\n   0.     0.     1.     0.     0.     1.     0.     0.     1.     0.\n   1.     0.     0.     0.     1.  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5193eb45065e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0macc_sk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vals_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vals_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mguess\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0macc_sk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_vals_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_sk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rachel/anaconda2/envs/tensorflow/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rachel/anaconda2/envs/tensorflow/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \"\"\"\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rachel/anaconda2/envs/tensorflow/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'support_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rachel/anaconda2/envs/tensorflow/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[  0.    58.67   4.46   3.04   1.     1.     6.     0.    43.   560.\n   0.     0.     1.     0.     0.     1.     0.     0.     1.     0.\n   1.     0.     0.     0.     1.  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "acc_sk = 0.0\n",
    "for x,y in zip(x_vals_test, y_vals_test):\n",
    "    guess = svm.predict(x)\n",
    "    if guess == y: acc_sk += 1./len(y_vals_test)\n",
    "print \"Accuracy: \", acc_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
